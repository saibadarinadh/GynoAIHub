{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
      "C:\\Users\\Badari\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_data[target] = le_targets[target].fit_transform(target_data[target])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Disorder: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Disorder: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Disorder:\n",
      "[[105   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0 104   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0 102   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0 110   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 114   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 104   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  96   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 102   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 101   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 103   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 119   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  97   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  92   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  80   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  91   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  89   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 107\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  101   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 110   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 108   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  85   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  94]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Treatment Recommendation: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Treatment Recommendation: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Treatment Recommendation:\n",
      "[[101   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  89   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0  94   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0  85   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0 102   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0 105   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0 101   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0  96   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  80   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 119   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 317   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 114   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 215   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  91   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  97   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 102   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 104\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   99   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 202]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Precautions: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Precautions: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Precautions:\n",
      "[[101   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0  91   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  99   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0 102   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0  80   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  94   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  97   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 105   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 110   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 108   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 107   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  89   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  96   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 102   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 110   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  87   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  92\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  104   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 101   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 114   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0 103   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 119   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 104]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Food Intake Recommendation: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Food Intake Recommendation: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Food Intake Recommendation:\n",
      "[[104   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  96   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0 209   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0 119   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0  89   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0 102   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0 107   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0 108   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0  92   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 104   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 101   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 415   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  80   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 114   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 102   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  91   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 101\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  179]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Foods to Avoid: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Foods to Avoid: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Foods to Avoid:\n",
      "[[ 97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 217   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 217   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 110   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 282   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 104   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  96   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 119   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 102   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  87   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 388   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 108   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  92   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 101   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  80]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Duration of Symptoms: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Duration of Symptoms: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Duration of Symptoms:\n",
      "[[104   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 204   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 321   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 114   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 103   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 105   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 101   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 182   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 195   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 713   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  91   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  87   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  80]]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters for SVM for target Lifestyle Recommendations: {'C': 0.1, 'gamma': 'scale'}\n",
      "Best parameters for Random Forest for target Lifestyle Recommendations: {'max_depth': None, 'n_estimators': 50}\n",
      "Confusion Matrix for Lifestyle Recommendations:\n",
      "[[ 87   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0 102   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0  97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0  99   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0 104   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0  85   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0  94   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0 195   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 105   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 215   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  89   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 195   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  96   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 110   0   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 110   0   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 102   0   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 101   0\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 101\n",
      "    0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  114   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 119   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  80]]\n",
      "\n",
      "Ensemble Model Metrics:\n",
      "Disorder:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Treatment Recommendation:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Precautions:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Food Intake Recommendation:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Foods to Avoid:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Duration of Symptoms:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "Lifestyle Recommendations:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n"
     ]
    },
    {
     "ename": "FileDataError",
     "evalue": "'.' is no file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileDataError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;31m# User input for PDF file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[0mpdf_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please enter the path to the PDF file: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;31m# Extract symptoms from the PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m \u001b[0mextracted_symptoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;31m# Predict outputs based on the extracted symptoms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[0mpredicted_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_symptoms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14828\\3818181166.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mfitz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpdf_document\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpdf_document\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   3066\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_pdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3069\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3070\u001b[1;33m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileDataError\u001b[0m: '.' is no file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pickle\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('gynecological_conditions.csv')\n",
    "\n",
    "# Define input features and targets\n",
    "X = data['Symptoms']  # Input features (Symptoms)\n",
    "\n",
    "# Targets for prediction\n",
    "targets = [\n",
    "    'Disorder',\n",
    "    'Treatment Recommendation',\n",
    "    'Precautions',\n",
    "    'Food Intake Recommendation',\n",
    "    'Foods to Avoid',\n",
    "    'Duration of Symptoms',\n",
    "    'Lifestyle Recommendations'\n",
    "]\n",
    "\n",
    "# Encode the target columns\n",
    "target_data = data[targets]\n",
    "le_targets = {}\n",
    "for target in targets:\n",
    "    le_targets[target] = LabelEncoder()\n",
    "    target_data[target] = le_targets[target].fit_transform(target_data[target])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train = target_data.loc[X_train.index]\n",
    "y_test = target_data.loc[X_test.index]\n",
    "\n",
    "# Vectorize the symptoms data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize classifiers\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "svm = SVC(probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Dictionary to store final ensemble models for each target\n",
    "models = {}\n",
    "ensemble_metrics = {}\n",
    "\n",
    "# Train and evaluate ensemble models with metrics\n",
    "for target in targets:\n",
    "    # Tune each model separately for the current target\n",
    "    grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=3, n_jobs=-1, verbose=1)\n",
    "    grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit models using GridSearchCV to find best parameters\n",
    "    grid_search_svm.fit(X_train_vectorized, y_train[target])\n",
    "    grid_search_rf.fit(X_train_vectorized, y_train[target])\n",
    "\n",
    "    # Retrieve the best models\n",
    "    best_svm = grid_search_svm.best_estimator_\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    print(f\"Best parameters for SVM for target {target}:\", grid_search_svm.best_params_)\n",
    "    print(f\"Best parameters for Random Forest for target {target}:\", grid_search_rf.best_params_)\n",
    "\n",
    "    # Voting classifier for ensemble model\n",
    "    voting_model = VotingClassifier(estimators=[\n",
    "        ('logreg', logreg),\n",
    "        ('svm', best_svm),\n",
    "        ('rf', best_rf)\n",
    "    ], voting='soft')\n",
    "    \n",
    "    # Train ensemble model for each target\n",
    "    voting_model.fit(X_train_vectorized, y_train[target])\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = voting_model.predict(X_test_vectorized)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test[target], y_pred)\n",
    "    precision = precision_score(y_test[target], y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test[target], y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test[target], y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    # Store metrics\n",
    "    ensemble_metrics[target] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "    models[target] = voting_model  # Store trained ensemble model\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test[target], y_pred)\n",
    "    print(f\"Confusion Matrix for {target}:\\n{cm}\")\n",
    "\n",
    "# Print ensemble model metrics\n",
    "print(\"\\nEnsemble Model Metrics:\")\n",
    "for target, metrics in ensemble_metrics.items():\n",
    "    print(f\"{target}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Save the trained ensemble models and vectorizer to files\n",
    "with open('ensemble_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(models, model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n",
    "\n",
    "# Prediction function using the ensemble model\n",
    "def predict_outputs(symptoms):\n",
    "    # Vectorize the input symptoms\n",
    "    input_vectorized = vectorizer.transform([symptoms])\n",
    "    predictions = {}\n",
    "    for target in targets:\n",
    "        predicted_encoded = models[target].predict(input_vectorized)\n",
    "        predictions[target] = le_targets[target].inverse_transform(predicted_encoded)[0]\n",
    "    return predictions\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        for page in pdf_document:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# User input for PDF file\n",
    "pdf_file_path = input(\"Please enter the path to the PDF file: \")\n",
    "\n",
    "# Extract symptoms from the PDF\n",
    "extracted_symptoms = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Predict outputs based on the extracted symptoms\n",
    "predicted_outputs = predict_outputs(extracted_symptoms)\n",
    "\n",
    "# Print the predicted outputs in the specified format\n",
    "print(f\"Based on extracted symptoms from '{pdf_file_path}':\")\n",
    "print(f\"Disorder: {predicted_outputs['Disorder']}\")\n",
    "print(f\"Treatment Recommendation: {predicted_outputs['Treatment Recommendation']}\")\n",
    "print(f\"Precautions: {predicted_outputs['Precautions']}\")\n",
    "print(f\"Recommended Food Intake: {predicted_outputs['Food Intake Recommendation']}\")\n",
    "print(f\"Foods to Avoid: {predicted_outputs['Foods to Avoid']}\")\n",
    "print(f\"Duration of Symptoms: {predicted_outputs['Duration of Symptoms']}\")\n",
    "print(f\"Lifestyle Recommendations: {predicted_outputs['Lifestyle Recommendations']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
